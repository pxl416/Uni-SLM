# train.py
import os
import time
import logging
from argparse import Namespace
from contextlib import nullcontext

import numpy as np
import torch
import matplotlib.pyplot as plt
from tqdm import tqdm
import wandb

import torch.nn as nn
import torch.nn.functional as F
from torch.cuda.amp import GradScaler, autocast
from torch.optim import Adam
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.nn.utils import clip_grad_norm_

from utils.config import load_config, load_train_config
from utils.dataset2 import create_dataloader
from utils.loss import build_loss, cosine_similarity_matrix
from models.Encoder.rgb_encoder import RGBEncoder
from models.Encoder.text_encoder import TextEncoder
from models.Head.retrieval import RetrievalHead
from models.Head.recognition import RecognitionHeadCTC
from models.Head.translation import TranslationHeadMT5

import os
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")


# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def set_seed(s: int = 42):
    """è®¾ç½®éšæœºç§å­"""
    import random
    random.seed(s)
    np.random.seed(s)
    torch.manual_seed(s)
    torch.cuda.manual_seed_all(s)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def cfg_get(ns, path, default=None):
    """
    å®‰å…¨è·å–åµŒå¥—é…ç½®ï¼›ns æ˜¯ SimpleNamespace æˆ– dictï¼›path ç”¨ 'A.B.C'ã€‚
    ä¾‹å­ï¼šcfg_get(self.config, "Evaluation.retrieval.enabled", False)
    """
    cur = ns
    for key in path.split("."):
        if cur is None:
            return default
        if isinstance(cur, dict):
            cur = cur.get(key, default if key == path.split(".")[-1] else None)
        else:
            if not hasattr(cur, key):
                return default
            cur = getattr(cur, key)
    return cur



class Trainer:
    def __init__(self, config):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.config = config
        self.scaler = GradScaler(enabled=torch.cuda.is_available())
        self.best_metric = 0.0

        self.setup_directories()
        self.setup_models()
        self.setup_optimizers()
        self.setup_loss()
        self.load_data()

    def setup_directories(self):
        """åˆå§‹åŒ–ä¿å­˜ç›®å½•"""
        cfg_dir = getattr(self.config, "save_dir", None)
        if cfg_dir and isinstance(cfg_dir, str) and cfg_dir != "PATH":
            base_dir = cfg_dir
        else:
            run_id = getattr(getattr(wandb, "run", None), "id", None)
            if not run_id:
                run_id = time.strftime("%Y%m%d-%H%M%S")
            base_dir = os.path.join("saved_models", run_id)

        self.save_dir = base_dir
        self.plot_dir = os.path.join(self.save_dir, "plots")
        os.makedirs(self.plot_dir, exist_ok=True)
        logger.info(f"Save directory: {self.save_dir}")

    def setup_models(self):
        self.models = {}
        self.eval_heads = {}

        # ====== 1) å¯è®­ç»ƒä¸»ç¼–ç å™¨ ======
        Dv = 512
        self.models["rgb"] = RGBEncoder(pretrained=True, output_dim=Dv).to(self.device)
        logger.info("Initialized RGBEncoder")

        # ä»…å½“å¯¹æ¯”å­¦ä¹ éœ€è¦æ–‡æœ¬åˆ†æ”¯
        if cfg_get(self.config, "Pretraining.task", "contrastive") == "contrastive":
            self.models["text"] = TextEncoder().to(self.device)
            logger.info("Initialized TextEncoder for contrastive learning")

        # ====== 2) é¢„è®­ç»ƒç”¨ projectorï¼ˆå¯è®­ç»ƒï¼‰ ======
        text_dim = getattr(self.models.get("text", None), "embedding_dim", 384)
        proj_dim = cfg_get(self.config, "Pretraining.projection_dim", 256)

        self.proj = nn.ModuleDict({
            "rgb": nn.Linear(Dv, proj_dim),
            "text": nn.Linear(text_dim, proj_dim),
        }).to(self.device)

        # åˆå§‹åŒ–ï¼ˆå¯é€‰ï¼‰
        for m in self.proj.values():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

        logger.info(f"Initialized pretraining projector: rgb {Dv}->{proj_dim}, text {text_dim}->{proj_dim}")

        # ====== 3) å†»ç»“è¯„ä¼°å¤´ï¼ˆåªåœ¨éªŒè¯ç”¨ï¼‰ ======
        # Retrieval
        if cfg_get(self.config, "Evaluation.retrieval.enabled", False):
            from models.Head.retrieval import RetrievalHead
            self.eval_heads["retrieval"] = RetrievalHead(
                rgb_in=Dv,
                text_in=text_dim,
                proj_dim=cfg_get(self.config, "Evaluation.retrieval.proj_dim", 256),
                temperature=cfg_get(self.config, "Evaluation.retrieval.temperature", 0.07),
            ).to(self.device)
            logger.info("Initialized RetrievalHead (frozen)")  # âœ… åªæœ‰è¿™ä¸€è¡Œ

        # Recognition
        if cfg_get(self.config, "Evaluation.recognition.enabled", False):
            from models.Head.recognition import RecognitionHeadCTC
            self.eval_heads["recognition"] = RecognitionHeadCTC(
                in_dim=Dv,
                num_classes=cfg_get(self.config, "Evaluation.recognition.num_classes", 2000),
                hidden=cfg_get(self.config, "Evaluation.recognition.hidden_dim", 512),
                nlayer=cfg_get(self.config, "Evaluation.recognition.num_layers", 2),
                dropout=cfg_get(self.config, "Evaluation.recognition.dropout", 0.1),
                blank_id=cfg_get(self.config, "Evaluation.recognition.blank_id", None),
            ).to(self.device)
            logger.info("Initialized RecognitionHeadCTC (frozen)")  # âœ… åªæœ‰è¿™ä¸€è¡Œ

        # Translation
        if cfg_get(self.config, "Evaluation.translation.enabled", False):
            from models.Head.translation import TranslationHeadMT5
            self.eval_heads["translation"] = TranslationHeadMT5(
                mt5_path=cfg_get(self.config, "Evaluation.translation.mt5_path", "google/mt5-base"),
                in_dim=Dv,
                d_model=cfg_get(self.config, "Evaluation.translation.hidden_dim", 768),
                label_smoothing=cfg_get(self.config, "Evaluation.translation.label_smoothing", 0.1),
                lang_prompt=cfg_get(self.config, "Evaluation.translation.lang", "Chinese"),
            ).to(self.device)
            logger.info("Initialized TranslationHeadMT5 (frozen)")

        logger.info(
            f"Trainable models: {list(self.models.keys())}; "
            f"Eval heads: {list(self.eval_heads.keys())}; "
            f"Pretraining.task={cfg_get(self.config, 'Pretraining.task', None)}"
        )

    def setup_optimizers(self):
        params = []
        for m in self.models.values():
            params += list(m.parameters())
        # projector å‚æ•°
        params += list(self.proj.parameters())

        if not params:
            raise ValueError("No trainable parameters found!")

        lr = cfg_get(self.config, "Training.learning_rate", 1e-4)
        opt_type = cfg_get(self.config, "optimizer.type", "adam")
        if opt_type == "adam":
            self.optimizer = Adam(params, lr=lr)
            logger.info(f"Initialized Adam optimizer with lr={lr}")
        else:
            raise ValueError(f"Unsupported optimizer: {opt_type}")

        if cfg_get(self.config, "optimizer.scheduler") == "cosine":
            self.scheduler = CosineAnnealingLR(
                self.optimizer,
                T_max=cfg_get(self.config, "Training.epochs", 20),
                eta_min=lr * 0.01,
            )
            logger.info("Initialized CosineAnnealingLR scheduler")
        else:
            self.scheduler = None

    def setup_loss(self):
        self.loss_fn = build_loss(self.config)
        logger.info(f"Initialized loss function: {cfg_get(self.config, 'Pretraining.task', 'contrastive')}")


    def load_data(self):
        """åŠ è½½æ•°æ®"""
        args = Namespace(
            dataset_name="CSL_News",
            batch_size=cfg_get(self.config, "Training.batch_size", 4),
            num_workers=2,
            max_length=64,
            rgb_support=True,
        )

        # å°è¯•åŠ è½½æ•°æ®é…ç½®ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ä¸»é…ç½®
        try:
            cfg = load_config("config/data_config.yaml")
        except:
            cfg = self.config
            logger.warning("Failed to load data_config.yaml, using main config")

        self.train_loader = create_dataloader(args, cfg, phase="train")
        self.val_loader = create_dataloader(args, cfg, phase="val")
        logger.info(f"Loaded data: {len(self.train_loader)} train batches, {len(self.val_loader)} val batches")

    def train_epoch(self, epoch: int):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        # è®¾ç½®æ¨¡å¼ï¼šä¸»æ¨¡å‹è®­ç»ƒï¼Œè¯„ä¼°å¤´è¯„ä¼°
        for model in self.models.values():
            model.train()
        for eval_head in self.eval_heads.values():
            eval_head.eval()

        total_loss = 0.0
        # progress_bar = tqdm(self.train_loader, desc=f"Epoch {epoch + 1}/{getattr(self.config.Training, 'epochs', 20)}")
        epochs = int(cfg_get(self.config, "Training.epochs", 20))
        progress_bar = tqdm(self.train_loader, desc=f"Epoch {epoch + 1}/{epochs}")

        # æ”¶é›†æ‰€æœ‰å¯è®­ç»ƒå‚æ•°
        # trainable_params = []
        # for model in self.models.values():
        #     trainable_params.extend(list(model.parameters()))

        # for batch_idx, (src_input, tgt_input) in enumerate(progress_bar):
            # è‡ªåŠ¨æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡
            # ctx = autocast() if torch.cuda.is_available() else nullcontext()

            # with ctx:
            #     loss = self.compute_pretrain_loss(src_input, tgt_input)
            #
            # # åå‘ä¼ æ’­å’Œä¼˜åŒ–
            # self.optimizer.zero_grad(set_to_none=True)
            # self.scaler.scale(loss).backward()
            # # clip_grad_norm_(trainable_params, max_norm=getattr(self.config.Training, 'gradient_clip', 1.0))
            # trainable_params.extend(list(self.proj.parameters()))
            # clip_grad_norm_(trainable_params, max_norm=cfg_get(self.config, 'Training.gradient_clip', 1.0))
            #
            # self.scaler.step(self.optimizer)
            # self.scaler.update()

        # æ”¶é›†æ‰€æœ‰å¯è®­ç»ƒå‚æ•°
        trainable_params = []
        for model in self.models.values():
            trainable_params.extend(list(model.parameters()))
        trainable_params.extend(list(self.proj.parameters()))  # â† åŠ åœ¨è¿™é‡Œï¼Œåˆ«æ”¾å¾ªç¯é‡Œ

        for batch_idx, (src_input, tgt_input) in enumerate(progress_bar):
            # è‡ªåŠ¨æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡
            ctx = autocast() if torch.cuda.is_available() else nullcontext()
            with ctx:
                loss = self.compute_pretrain_loss(src_input, tgt_input)

            self.optimizer.zero_grad(set_to_none=True)
            self.scaler.scale(loss).backward()
            clip_grad_norm_(trainable_params, max_norm=cfg_get(self.config, 'Training.gradient_clip', 1.0))
            self.scaler.step(self.optimizer)
            self.scaler.update()


            total_loss += loss.item()
            progress_bar.set_postfix(loss=loss.item())

            # è®°å½•åˆ°wandb
            if wandb.run and batch_idx % 10 == 0:
                wandb.log({
                    "train/batch_loss": loss.item(),
                    "epoch": epoch,
                    "batch": batch_idx,
                    "learning_rate": self.optimizer.param_groups[0]["lr"]
                })

        return total_loss / max(1, len(self.train_loader))

    def compute_pretrain_loss(self, src_input, tgt_input):
        """è®¡ç®—é¢„è®­ç»ƒä¸»æŸå¤±"""
        task = cfg_get(self.config, 'Pretraining.task', 'contrastive')

        if task == "contrastive":
            # è§†è§‰ç‰¹å¾ [B,T,D] â†’ [B,D]
            rgb_feat = self.models["rgb"](src_input["rgb_img"].to(self.device))
            rgb_pooled = rgb_feat.mean(dim=1)  # [B, 512]

            if "text" not in self.models:
                raise ValueError("TextEncoder required for contrastive learning")

            text_out, attn_mask = self.models["text"](tgt_input["gt_sentence"])

            # æ–‡æœ¬ç‰¹å¾ï¼šå¥å‘é‡ or åºåˆ—å¹³å‡
            if text_out.ndim == 2:
                text_pooled = text_out  # [B, Dt]
            else:
                mask = attn_mask.unsqueeze(-1)  # [B,L,1]
                valid_mask = (mask.sum(dim=1) > 0).squeeze(-1)  # [B]
                if not valid_mask.any():
                    # é˜²æ­¢å‡ºç°ç©º batch
                    return torch.tensor(0.0, device=self.device, requires_grad=True)
                rgb_pooled = rgb_pooled[valid_mask]
                text_feat = text_out[valid_mask]
                mask = mask[valid_mask]
                text_pooled = (text_feat * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-5)  # [B', Dt]

            # ---- å…³é”®ï¼šä¸¤ä¾§æŠ•å½±åˆ°åŒä¸€ç»´åº¦ï¼Œå†å½’ä¸€åŒ–ï¼Œå†åšç›¸ä¼¼åº¦ ----
            v = F.normalize(self.proj["rgb"](rgb_pooled), dim=1)  # [B', P]
            t = F.normalize(self.proj["text"](text_pooled), dim=1)  # [B', P]
            sim_matrix = v @ t.T  # [B', B']

            temperature = cfg_get(self.config, "Pretraining.temperature", 0.07)
            return self.loss_fn({'sim_matrix': sim_matrix, 'temperature': temperature})

        else:
            raise ValueError(f"Unsupported pretraining task: {task}")

    @torch.no_grad()
    def validate(self, epoch: int = None):
        """éªŒè¯ï¼šåœ¨æ‰€æœ‰ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¯„ä¼°"""
        results = {}

        # å¼€å§‹éªŒè¯çš„è¯¦ç»†æ—¥å¿—
        logger.info(f"ğŸš€ Starting validation for epoch {epoch}...")

        # è®¾ç½®æ¨¡å‹æ¨¡å¼
        for name, model in self.models.items():
            model.eval()
            logger.debug(f"Set {name} model to eval mode")

        for name, eval_head in self.eval_heads.items():
            eval_head.eval()
            logger.debug(f"Set {name} head to eval mode")

        # åˆ†åˆ«è¯„ä¼°æ¯ä¸ªä»»åŠ¡å¹¶è®°å½•è¯¦ç»†ä¿¡æ¯
        task_results = {}

        # Retrieval è¯„ä¼°
        if "retrieval" in self.eval_heads:
            logger.info("ğŸ“Š Evaluating Retrieval task...")
            start_time = time.time()
            try:
                retrieval_metrics = self.evaluate_retrieval()
                task_results["retrieval"] = retrieval_metrics
                results.update(retrieval_metrics)
                elapsed = time.time() - start_time

                # è¯¦ç»†è®°å½•æ£€ç´¢æŒ‡æ ‡
                r1 = retrieval_metrics.get("retrieval/R1", 0)
                r5 = retrieval_metrics.get("retrieval/R5", 0)
                r10 = retrieval_metrics.get("retrieval/R10", 0)
                # mean_r1 = retrieval_metrics.get("retrieval/mean_R1", 0)
                mean_r1 = retrieval_metrics.get("retrieval/mean_R1", None)
                if mean_r1 is None:
                    r1_i2t = retrieval_metrics.get("retrieval/i2t_R1", 0.0)
                    r1_t2i = retrieval_metrics.get("retrieval/t2i_R1", 0.0)
                    mean_r1 = 0.5 * (r1_i2t + r1_t2i)

                logger.info(f"   Retrieval Results - R1: {r1:.2%}, R5: {r5:.2%}, R10: {r10:.2%}, MeanR1: {mean_r1:.2%}")
                logger.info(f"   Retrieval evaluation completed in {elapsed:.2f}s")

            except Exception as e:
                logger.error(f"âŒ Retrieval evaluation failed: {e}")
                import traceback
                logger.error(traceback.format_exc())

        # Recognition è¯„ä¼°
        if "recognition" in self.eval_heads:
            logger.info("ğŸ“Š Evaluating Recognition task...")
            start_time = time.time()
            try:
                recognition_metrics = self.evaluate_recognition()
                task_results["recognition"] = recognition_metrics
                results.update(recognition_metrics)
                elapsed = time.time() - start_time

                ctc_loss = recognition_metrics.get("recognition/ctc_loss", float('inf'))
                logger.info(f"   Recognition CTC Loss: {ctc_loss:.4f}")
                logger.info(f"   Recognition evaluation completed in {elapsed:.2f}s")

            except Exception as e:
                logger.error(f"âŒ Recognition evaluation failed: {e}")

        # Translation è¯„ä¼°
        if "translation" in self.eval_heads:
            logger.info("ğŸ“Š Evaluating Translation task...")
            start_time = time.time()
            try:
                translation_metrics = self.evaluate_translation()
                task_results["translation"] = translation_metrics
                results.update(translation_metrics)
                elapsed = time.time() - start_time

                # è®°å½•ç¿»è¯‘æŒ‡æ ‡
                for metric_name, value in translation_metrics.items():
                    if "loss" in metric_name:
                        logger.info(f"   {metric_name}: {value:.4f}")
                    else:
                        logger.info(f"   {metric_name}: {value:.2f}")
                logger.info(f"   Translation evaluation completed in {elapsed:.2f}s")

            except Exception as e:
                logger.error(f"âŒ Translation evaluation failed: {e}")

        # æ±‡æ€»æ—¥å¿—
        logger.info(f"ğŸ¯ === Epoch {epoch} Validation Summary ===")
        for task_name, metrics in task_results.items():
            logger.info(f"   {task_name.upper()}:")
            for metric_name, value in metrics.items():
                if isinstance(value, (int, float)):
                    if 0 <= value <= 1:  # ç™¾åˆ†æ¯”æ ¼å¼
                        logger.info(f"     {metric_name}: {value:.2%}")
                    else:  # æ™®é€šæ•°å€¼
                        logger.info(f"     {metric_name}: {value:.4f}")
                else:
                    logger.info(f"     {metric_name}: {value}")
        r1 = retrieval_metrics.get("retrieval/R1", None)
        r5 = retrieval_metrics.get("retrieval/R5", None)
        r10 = retrieval_metrics.get("retrieval/R10", None)

        # è‹¥æ²¡æœ‰å•å‘æ±‡æ€»é”®ï¼Œåˆ™ç”¨åŒå‘å¹³å‡
        def avg(key):
            a = retrieval_metrics.get(f"retrieval/i2t_{key}", None)
            b = retrieval_metrics.get(f"retrieval/t2i_{key}", None)
            if a is None and b is None: return None
            if a is None: return b
            if b is None: return a
            return 0.5 * (a + b)

        if r1 is None:  r1 = avg("R1")
        if r5 is None:  r5 = avg("R5")
        if r10 is None:  r10 = avg("R10")

        mean_r1 = retrieval_metrics.get("retrieval/mean_R1", None)
        if mean_r1 is None:
            i2t = retrieval_metrics.get("retrieval/i2t_R1", 0.0)
            t2i = retrieval_metrics.get("retrieval/t2i_R1", 0.0)
            mean_r1 = 0.5 * (i2t + t2i)

        logger.info(
            f"   Retrieval Results - R1: {(r1 or 0):.2%}, R5: {(r5 or 0):.2%}, R10: {(r10 or 0):.2%}, MeanR1: {mean_r1:.2%}")

        # è®°å½•åˆ°wandb
        if wandb.run:
            wandb.log(results, step=epoch)

        return results

    @torch.no_grad()
    def evaluate_retrieval(self):
        """è¯„ä¼°æ£€ç´¢ä»»åŠ¡"""
        all_rgb, all_text = [], []

        for src_input, tgt_input in self.val_loader:
            # æå–ç‰¹å¾
            rgb_feat = self.models["rgb"](src_input["rgb_img"].to(self.device))
            rgb_pooled = rgb_feat.mean(dim=1)  # [B, D]

            if "text" in self.models:
                text_out, attn_mask = self.models["text"](tgt_input["gt_sentence"])

                # å¤„ç†æ–‡æœ¬ç‰¹å¾
                if text_out.ndim == 2:
                    text_pooled = text_out
                else:
                    mask = attn_mask.unsqueeze(-1)
                    text_pooled = (text_out * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-5)

                all_rgb.append(rgb_pooled)
                all_text.append(text_pooled)

        if all_rgb and all_text:
            rgb_features = torch.cat(all_rgb, dim=0)
            text_features = torch.cat(all_text, dim=0)

            return self.eval_heads["retrieval"].compute_metrics(rgb_features, text_features)

        return {}

    @torch.no_grad()
    def evaluate_recognition(self):
        """è¯„ä¼°è¯†åˆ«ä»»åŠ¡ï¼ˆCTCï¼‰"""
        if "recognition" not in self.eval_heads:
            return {}

        total_loss, count = 0.0, 0

        for src_input, tgt_input in self.val_loader:
            # å¿…é¡»æœ‰æ ‡ç­¾
            if "gt_gloss" not in tgt_input:
                continue

            # è§†è§‰æ—¶åºç‰¹å¾ä¸ mask
            rgb_seq = self.models["rgb"](src_input["rgb_img"].to(self.device))  # [B,T,Dv]
            src_mask = src_input.get("attention_mask", None)
            if src_mask is None:
                # æ²¡æä¾›å°±é»˜è®¤å…¨æœ‰æ•ˆ
                src_mask = torch.ones(rgb_seq.size(0), rgb_seq.size(1), device=self.device, dtype=torch.long)

            # CTC éœ€è¦ True=pad çš„ mask
            src_key_padding_mask = (src_mask == 0)
            input_lengths = (~src_key_padding_mask).sum(dim=1).to(torch.long)  # [B]

            # ç›®æ ‡æ ‡ç­¾ï¼šå…¼å®¹ List[List[int]] æˆ– (targets, target_lengths)
            gt_gloss = tgt_input["gt_gloss"]
            if isinstance(gt_gloss, torch.Tensor) and gt_gloss.ndim == 1:
                # å·²æ˜¯æ‹¼æ¥å¥½çš„ä¸€ç»´ targetsï¼ŒåŒæ­¥è·å– lengths
                if "gt_gloss_lengths" not in tgt_input:
                    # æ²¡æœ‰ lengthsï¼Œæ— æ³•è®¡ç®— CTC
                    continue
                targets = gt_gloss.to(self.device)
                target_lengths = tgt_input["gt_gloss_lengths"].to(self.device).to(torch.long)
            else:
                # è®¤ä¸ºæ˜¯ List[List[int]]
                if not isinstance(gt_gloss, (list, tuple)) or not isinstance(gt_gloss[0], (list, tuple)):
                    # æ ¼å¼å¼‚å¸¸ï¼Œè·³è¿‡
                    continue
                target_lengths = torch.tensor([len(x) for x in gt_gloss], dtype=torch.long, device=self.device)
                flat = [tid for seq in gt_gloss for tid in seq]
                targets = torch.tensor(flat, dtype=torch.long, device=self.device)

            # å‰å‘ + loss
            logits = self.eval_heads["recognition"](rgb_seq, src_key_padding_mask=src_key_padding_mask)  # [B,T,V]
            loss = self.eval_heads["recognition"].compute_loss(
                logits, targets, input_lengths, target_lengths
            )
            total_loss += float(loss.item());
            count += 1

        return {"recognition/ctc_loss": (total_loss / max(1, count))} if count > 0 else {}

    @torch.no_grad()
    def evaluate_translation(self):
        """è¯„ä¼°ç¿»è¯‘ä»»åŠ¡"""
        if "translation" not in self.eval_heads:
            return {}

        sums = {}  # æŒ‡æ ‡å -> ç´¯åŠ å’Œ
        counts = {}  # æŒ‡æ ‡å -> ç´¯è®¡æ ·æœ¬/æ‰¹æ¬¡æ•°ï¼ˆè¿™é‡ŒæŒ‰æ‰¹è®¡æ•°ï¼‰

        for src_input, tgt_input in self.val_loader:
            if "gt_sentence" not in tgt_input:
                continue

            # æå–ç‰¹å¾
            rgb_seq = self.models["rgb"](src_input["rgb_img"].to(self.device))

            # è®¡ç®—æŒ‡æ ‡ï¼ˆTranslationHead.compute_metrics è¿”å›å¦‚ {"translation_loss":..., "perplexity":...}ï¼‰
            batch_metrics = self.eval_heads["translation"].compute_metrics(
                vis_seq=rgb_seq,
                tgt_texts=tgt_input["gt_sentence"]
            )

            # ç´¯ç§¯
            for k, v in batch_metrics.items():
                sums[k] = sums.get(k, 0.0) + float(v)
                counts[k] = counts.get(k, 0) + 1

        # å–å‡å€¼å¹¶åŠ å‰ç¼€
        out = {}
        for k in sums:
            out[f"translation/{k}"] = sums[k] / max(1, counts[k])
        return out

    def save_checkpoint(self, state: dict, filename: str):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        torch.save(state, filename)

        try:
            if wandb.run:
                wandb.save(filename)
        except Exception as e:
            logger.warning(f"Failed to upload to wandb: {e}")


    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        logger.info("Starting training...")

        epochs = int(cfg_get(self.config, "Training.epochs", 20))
        eval_freq = int(cfg_get(self.config, "Training.eval_freq", 1))

        for epoch in range(epochs):
            # 1) è®­ç»ƒä¸€ä¸ª epoch
            train_loss = self.train_epoch(epoch)
            logger.info(f"Epoch {epoch + 1}/{epochs}: train_loss = {train_loss:.4f}")

            if wandb.run:
                wandb.log({"train/epoch_loss": train_loss, "epoch": epoch})

            # 2) éªŒè¯ï¼ˆæŒ‰é¢‘ç‡ï¼‰
            if (epoch % eval_freq) == 0:
                eval_metrics = self.validate(epoch)

                # ç”¨æ£€ç´¢çš„ mean_R1 ä½œä¸ºâ€œæœ€ä½³æ¨¡å‹â€çš„åº¦é‡ï¼ˆè‹¥ä¸å­˜åœ¨åˆ™è·³è¿‡ï¼‰
                mean_r1 = eval_metrics.get("retrieval/mean_R1", None)
                if mean_r1 is not None and mean_r1 > self.best_metric:
                    self.best_metric = mean_r1

                    # ç»„è£… checkpoint
                    state = {
                        "epoch": epoch,
                        "best_metric": self.best_metric,
                        # æ³¨æ„ï¼švars(self.config) åªé€‚ç”¨äº Namespaceï¼›
                        # å¦‚æœæ˜¯ SimpleNamespace ä¹Ÿå¯ä»¥ï¼Œä½†è‹¥æ˜¯åµŒå¥—å¯¹è±¡å¤æ‚ï¼Œå»ºè®®è‡ªå·±åºåˆ—åŒ–ã€‚
                        "config": vars(self.config) if hasattr(self.config, "__dict__") else {},
                    }
                    for name, model in self.models.items():
                        state[f"{name}_state_dict"] = model.state_dict()
                    state["optimizer_state_dict"] = self.optimizer.state_dict()
                    if self.scheduler is not None:
                        state["scheduler_state_dict"] = self.scheduler.state_dict()

                    self.save_checkpoint(state, os.path.join(self.save_dir, "best_model.pt"))
                    logger.info(f"ğŸ”¥ New best model: mean_R1 = {self.best_metric:.4f}")

            # 3) å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler is not None:
                self.scheduler.step()

        logger.info("Training completed!")


def main():
    """ä¸»å‡½æ•°"""
    set_seed(42)

    # åŠ è½½é…ç½®
    cfg = load_train_config()
    logger.info("Loaded configuration")

    # åˆå§‹åŒ–W&B
    if getattr(cfg.wandb, 'use', False):
        try:
            wandb.init(
                project=getattr(cfg.wandb, 'project', 'sign-language-pretraining'),
                name=getattr(cfg.wandb, 'run_name', 'experiment'),
                config=vars(cfg)
            )
            logger.info("Initialized Weights & Biases")
        except Exception as e:
            logger.warning(f"W&B initialization failed: {e}")
            os.environ["WANDB_MODE"] = "disabled"
    else:
        os.environ["WANDB_MODE"] = "disabled"
        logger.info("W&B disabled")

    # å¼€å§‹è®­ç»ƒ
    trainer = Trainer(cfg)
    trainer.train()


if __name__ == "__main__":
    main()